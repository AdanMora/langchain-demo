{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9864082",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY=\"your_openai_api_key_here\"\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d616420",
   "metadata": {},
   "source": [
    "# What are Large Language Models (LLMs)?\n",
    "\n",
    "Large Language Models (LLMs) are artificial intelligence systems trained on vast amounts of text data to understand, generate, and manipulate human language. They use deep learning architectures, particularly transformers, to process and generate text by predicting the most likely next words in a sequence.\n",
    "\n",
    "**Important Note**: LLMs are trained at a specific point in time using data available up to that moment. As new information, events, and knowledge continue to emerge in the real world, the model's training data becomes increasingly outdated, creating a knowledge gap that grows over time.\n",
    "\n",
    "## Strengths of LLMs\n",
    "\n",
    "* üéØ **Versatility**\n",
    "* üß† **Language Understanding**\n",
    "* üöÄ **Accessibility**\n",
    "* üí° **Creative and Analytical**\n",
    "\n",
    "## Weaknesses and Limitations\n",
    "\n",
    "* ‚ùå **Hallucinations**\n",
    "* üìÖ **Knowledge Cutoff**\n",
    "* üé≤ **Inconsistency**\n",
    "* üîç **Lack of True Understanding**\n",
    "* ‚öñÔ∏è **Bias and Ethics**\n",
    "* üí∞ **Cost and Resources**\n",
    "\n",
    "## Summary:\n",
    "* Biggest issues isolation from the real world and memory.\n",
    "\n",
    "\n",
    "## Best Practices for Using LLMs\n",
    "\n",
    "1. **Verify Information**: Always fact-check important information\n",
    "2. **Use Clear Prompts**: Be specific and detailed in your requests\n",
    "3. **Implement Safeguards**: Add content filtering and validation\n",
    "4. **Monitor Costs**: Track API usage and implement usage limits\n",
    "5. **Combine with Other Tools**: Use LLMs as part of larger systems (like we'll see with LangChain agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0006fe",
   "metadata": {},
   "source": [
    "### Most basic interaction with a LLM an isolate call\n",
    "Using a LLM to suggest an Italian food name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c21aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.6)\n",
    "name = llm(\"I want to open a restaurant for Italian food. Suggest a fancy name for this.\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99651b49",
   "metadata": {},
   "source": [
    "### Using Langchain to create prompt template for differents cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(\n",
    "    input_variable = [\"cuisine\"],\n",
    "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fancy name for this.\"\n",
    ")\n",
    "\n",
    "prompt_template_name.format(cuisine=\"Italian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956ea327",
   "metadata": {},
   "source": [
    "### What is a Chain?\n",
    "A chain is a sequence of calls to components like LLMs, prompts, and other chains. It allows you to:\n",
    "\n",
    "* Link multiple operations together - Instead of making separate, isolated calls\n",
    "* Pass outputs from one step as inputs to the next - Creating a pipeline of operations\n",
    "* Build complex workflows - Combine simple operations into sophisticated applications\n",
    "* Reuse components - Create modular, reusable pieces of functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3294401",
   "metadata": {},
   "source": [
    "### Using Langchain chain\n",
    "We are creating a simple chain, ussing the llm we created, the prompt for restaraunt names and a why to identify the output with a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ef2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")\n",
    "name_chain.run(\"American\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c340d5",
   "metadata": {},
   "source": [
    "### Chain for menus \n",
    "Now we are creating another chain for menu suggestions for a restaurant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b328637f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_items = PromptTemplate(\n",
    "    input_variable = [\"restaurant_name\"],\n",
    "    template = \"Suggest some menu items for {restaurant_name}. Return it as list sepparate by comma.\"\n",
    ")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")\n",
    "food_items_chain.run(\"Mexican\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab478e",
   "metadata": {},
   "source": [
    "### Linking chains\n",
    "Now we want to use both chains to create a menu both using the restaurant name to before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d36d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains=[name_chain, food_items_chain])\n",
    "chain.run(\"Japanesse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bd1bb6",
   "metadata": {},
   "source": [
    "If you notice we only get the menu but not the restaurant name. We want both so we are going to use the output key to return both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7beef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(\n",
    "    chains=[name_chain, food_items_chain],\n",
    "    input_variables=[\"cuisine\"],\n",
    "    output_variables=[\"restaurant_name\", \"menu_items\"]\n",
    ")\n",
    "\n",
    "chain({\"cuisine\": \"Arabic\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c66d2",
   "metadata": {},
   "source": [
    "Observation: at this point we should be able to understand the basics of an ai agent interaction that is the linking of multiple operations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d059bc",
   "metadata": {},
   "source": [
    "## AI AGENTS\n",
    "AI Agents were created to solve this isolation problem by giving LLMs the ability to:\n",
    "\n",
    "* üîß Use Tools: Web search (Wikipedia, Google), Calculators for math, Database queries, API calls and File operations. \n",
    "* üß† Think and Plan: Break down complex problems into steps, Decide which tools to use and when, Chain together multiple actions and Reason about the results\n",
    "* üîÑ Act and React: Take actions based on information, Get feedback from tools, Adjust their approach based on results and Iterate until they solve the problem\n",
    "\n",
    "Summay: AI agent give the capabilities to execute tools that can help to fill some of the issues of isolation and to be able to store memory of actions or knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ed5e6d",
   "metadata": {},
   "source": [
    "### Tools\n",
    "A tool is a function or capability that an agent can use to interact with the external world. Tools are essentially functions with descriptions that:\n",
    "\n",
    "Perform specific tasks (search Wikipedia, do math, call APIs)\n",
    "Have clear descriptions so the agent knows when to use them\n",
    "Return results that the agent can reason about\n",
    "Can be combined to solve complex problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b412fa95",
   "metadata": {},
   "source": [
    "We are going to use some predifine tools like wikipedia and llm-math, then we are going to create an agent with this tools.\n",
    "\n",
    "Note: ZERO_SHOT_REACT_DESCRIPTION is a specific agent type in LangChain that defines how the agent thinks and acts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85728e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, load_tools\n",
    "\n",
    "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "agent.run(\"When was Albert Einstein born? What would be his age right now in 2026?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fdfdc5",
   "metadata": {},
   "source": [
    "We see that the LLM was able to solve the problem?\n",
    "\n",
    "How?\n",
    "\n",
    "So, now try to uncomment the `verbose=True` line and run it again.\n",
    "\n",
    "\n",
    "------------------------\n",
    "\n",
    "What we see is that the agent would do a react loop where:\n",
    "\n",
    "* Thought: \"I need to find Einstein's birth date and calculate his age\"\n",
    "* Action: Use Wikipedia tool to search \"Albert Einstein\"\n",
    "* Observation: \"Einstein was born March 14, 1879\"\n",
    "* Thought: \"Now I need to calculate 2026 - 1879\"\n",
    "* Action: Use llm-math tool to calculate the difference\n",
    "* Observation: \"The result is 147 years\"\n",
    "* Thought: \"I have both pieces of information needed\"\n",
    "* Final Answer: \"Albert Einstein was born on March 14, 1879. He would be 147 years old in 2026.\"\n",
    "\n",
    "Notes: \n",
    "* You probably would notice that the wikipedia information would bring to posible dates, here the LLM creativity would cause something to use one date or the other. \n",
    "* Also you probably would notice that the agent do not return the date and the years, only the year. That because the way we wrote the prompt and since the years is the last questions we only see it because of the step by step it does. If we change the prompt for a 1 questions linked with `and` instead of two questions we are going to see differents results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e050df16",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8032c966",
   "metadata": {},
   "source": [
    "We have been using the same LLM all this time, but does it recall what was the questions and conversations we had?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_question = llm(\"What was my previous question?\")\n",
    "print(last_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa38ee",
   "metadata": {},
   "source": [
    "If we check the chain we can see it have a memory property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ceb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811aa0e",
   "metadata": {},
   "source": [
    "Wich is empty.\n",
    "\n",
    "But we can set something there. Let's try using a buffer from langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
    "name = name_chain.run(\"French\")\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d121aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734bd062",
   "metadata": {},
   "source": [
    "This is the way that langchain adds memory to the Chains. \n",
    "\n",
    "But how it works? We can follow the next example using a Conversation Chain since I believe most of you have ever use ChatGpt, so it would be easier to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2e65a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "convo = ConversationChain(llm=llm)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"Who won the last Lacrosse tournament?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992f405",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"What is 5+5?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de9188",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.run(\"Who was the captain of the winning team?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f04a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.memory.buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5201fde6",
   "metadata": {},
   "source": [
    "At the end how the memory works is basically that it adds to the prompt it sents to the LLM the history of the conversation or the additional data stored on the memory to the LLM so it. \n",
    "\n",
    "So be carefull with the conversations or context you add since the price can increase depending on the input tokens and the output tokens."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
